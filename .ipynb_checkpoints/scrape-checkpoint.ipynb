{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "source": [
    "Collect the full HTML text of every resume listed in this link:\n",
    "http://www.indeed.com/resumes?q=%22data+scientist%22&co=US\n",
    "We need the FULL HTML of the page that you get after after you click on a resume link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.indeed.com/r/2c1339d118304c09?sp=0\n",
      "http://www.indeed.com/r/Hamel-Husain/f4ec22ad43cfb217?sp=0\n",
      "http://www.indeed.com/r/Hillary-Green/d9f8a04e09b9a2ff?sp=0\n",
      "http://www.indeed.com/r/Michael-Muratet/de56e368dbc9dbd5?sp=0\n",
      "http://www.indeed.com/r/Mustafa-Egilmez/a5162d053b5a1f33?sp=0\n",
      "http://www.indeed.com/r/Joseph-Daoud/8a75b90480fe0d03?sp=0\n",
      "http://www.indeed.com/r/Sai-Kasiraju/588b58ba7bdc191d?sp=0\n",
      "http://www.indeed.com/r/Leo-Data-Scientist/7f4081e6e3b074da?sp=0\n",
      "http://www.indeed.com/r/Shuang-Chen/b19222e57af95c7a?sp=0\n",
      "http://www.indeed.com/me/davidcomfort?sp=0\n",
      "http://www.indeed.com/r/Jie-Li/3748da17f5479d0c?sp=0\n",
      "http://www.indeed.com/r/Pavan-M/0d83d9618afc5f20?sp=0\n",
      "http://www.indeed.com/r/Dennis-Noren/9b0e168318f72f0a?sp=0\n",
      "http://www.indeed.com/r/John-Babb/2e33fe25bbc816b7?sp=0\n",
      "http://www.indeed.com/r/Jiang-(Nick)-Chen/470ab57dedbe30bf?sp=0\n",
      "http://www.indeed.com/r/Jeremy-Graybill/932719e238640dcf?sp=0\n",
      "http://www.indeed.com/r/Kristine-Baker/9863d4170e39496e?sp=0\n",
      "http://www.indeed.com/r/Arthur-Juliani/0c9079525a9da3ce?sp=0\n",
      "http://www.indeed.com/r/Rommel-Fernandes/c4d00455d47ab57e?sp=0\n",
      "http://www.indeed.com/r/Cody-Bushnell/ef13e89dc52bb346?sp=0\n",
      "http://www.indeed.com/r/Eric-Smith/24dd751bcf9c5fcf?sp=0\n",
      "http://www.indeed.com/r/Rohan-Adur/5dc9b3304ba398d3?sp=0\n",
      "http://www.indeed.com/r/Keith-Wheeles/810a6f8a1f6ccd2e?sp=0\n",
      "http://www.indeed.com/r/Daniel-Lopez-Benetton/cd7fbab0ac7cde10?sp=0\n",
      "http://www.indeed.com/r/Jason-Sypniewski/c3ec5fcf60a8b529?sp=0\n",
      "http://www.indeed.com/r/Christopher-Mayers/6e2ebb2cf85c4c91?sp=0\n",
      "http://www.indeed.com/r/Evgenii-Konev/9cf438d5befdd0c2?sp=0\n",
      "http://www.indeed.com/r/Ziddun-Ansari/71f6469018c7a244?sp=0\n",
      "http://www.indeed.com/me/ochoa_pedro?sp=0\n",
      "http://www.indeed.com/r/Pankaj-Singhal/fbd48069543891c9?sp=0\n",
      "http://www.indeed.com/r/Ke-Zhou/449a411b0cc70258?sp=0\n",
      "http://www.indeed.com/r/Scott-Clendaniel/3d8a9c590bc4c2be?sp=0\n",
      "http://www.indeed.com/r/Eric-Volstromer/23286b11451977f5?sp=0\n",
      "http://www.indeed.com/r/John-Case/80649152f75ab936?sp=0\n",
      "http://www.indeed.com/r/Yibing-Qi/f19c4105129abc20?sp=0\n",
      "http://www.indeed.com/r/Yixuan-Guo/e01e2d01aaaec61b?sp=0\n",
      "http://www.indeed.com/r/Kevan-Rajaram/a84f176307178142?sp=0\n",
      "http://www.indeed.com/r/Alex-Talalayevsky/7a89e50762148472?sp=0\n",
      "http://www.indeed.com/r/Travis-Adam-Rose/39bb58f6b4d0a34c?sp=0\n",
      "http://www.indeed.com/r/W-Doughty/e73c0534d9232a7a?sp=0\n",
      "http://www.indeed.com/r/Zongyan-Wang/ff9b8cd1c21541ea?sp=0\n",
      "http://www.indeed.com/r/Edana-Merchan/c1ace0818905f4f6?sp=0\n",
      "http://www.indeed.com/r/Praveen-Kumar/3df37b797c646fbf?sp=0\n",
      "http://www.indeed.com/r/Lakshminarayanan-Edapalapatty/3fcd06db909c16a7?sp=0\n",
      "http://www.indeed.com/r/Cong-Wu/52329cc35e4160f2?sp=0\n",
      "http://www.indeed.com/r/Xiao-Huang/b2e24175122e16f7?sp=0\n",
      "http://www.indeed.com/r/Kishore-P/e2ac460d695e9da9?sp=0\n",
      "http://www.indeed.com/r/David-Ziganto/922894e51c5f803e?sp=0\n",
      "http://www.indeed.com/r/Anish-Acharya/e3e457afea1b136d?sp=0\n",
      "http://www.indeed.com/r/Edward-Fine/fbe1049a6471384e?sp=0\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import *\n",
    "from bs4 import BeautifulSoup\n",
    "resume = urlopen('http://www.indeed.com/resumes?q=%22data+scientist%22&co=US')\n",
    "bsObj = BeautifulSoup(resume, 'lxml')\n",
    "for link in bsObj.findAll(\"a\", {\"target\" : \"_blank\", \"rel\" : \"nofollow\", \"class\" : \"app_link\", \"itemprop\" : \"url\"}):\n",
    "    newlink = 'http://www.indeed.com' + link.attrs['href']\n",
    "    print(newlink)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each user on Kaggle: (https://www.kaggle.com/users) make a new user folder and add the following inside:\n",
    "- An html with a profile page (https://www.kaggle.com/titericz/)\n",
    "- An html with the results (https://www.kaggle.com/titericz/results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from urllib.request import urlretrieve \n",
    "from urllib.request import urlopen \n",
    "from bs4 import BeautifulSoup\n",
    "html = urlopen(\"https://www.kaggle.com/users\")\n",
    "bsObj = BeautifulSoup(html, \"lxml\")\n",
    "for link in bsObj.findAll(\"a\", {\"class\":\"profilelink\"}):\n",
    "    newpath = \"/Users/zh355245849/Desktop/project6/\" + link.contents[0]\n",
    "    if not os.path.exists(newpath):\n",
    "        os.makedirs(newpath)\n",
    "    response = urlopen('https://www.kaggle.com' + link.attrs['href'])\n",
    "    f = open(newpath + \"/\" + link.contents[0] + \".html\",'wb')\n",
    "    page = response.read()\n",
    "    f.write(page)\n",
    "    f.close\n",
    "    result = urlopen('https://www.kaggle.com' + link.attrs['href'] + \"/results\")\n",
    "    f1 = open(newpath + \"/\" + \"resultPage\" + \".html\",'wb')\n",
    "    page1 = result.read()\n",
    "    f1.write(page1)\n",
    "    f1.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each company on careercup (http://www.careercup.com/categories):\n",
    "- Make a new folder for the company\n",
    "- Insider the company folder, make a new HTML file for each question for the company.\n",
    "There might be multiple pages of questions for the company. We want all of them. \n",
    "Each file should include the HTML of the page that you get when you click on a question.\n",
    "Example: http://www.careercup.com/question?id=3397693"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Project 7:\n",
    "import os\n",
    "from urllib.request import urlretrieve \n",
    "from urllib.request import urlopen \n",
    "from bs4 import BeautifulSoup\n",
    "html = urlopen(\"http://www.careercup.com/categories\")\n",
    "bsObj = BeautifulSoup(html, \"lxml\")\n",
    "# os.makedirs('/Users/zh355245849/Desktop/project7')\n",
    "for link in bsObj.find('div', {\"class\":\"box\"}).findAll('a'):\n",
    "    if '/' in link.contents[0]:\n",
    "        strs = link.contents[0].split('/')\n",
    "        link.contents[0] = ''\n",
    "        for s in strs:\n",
    "            link.contents[0] += s\n",
    "    if not os.path.exists('/Users/zh355245849/Desktop/project7/' + link.contents[0]):\n",
    "        os.makedirs('/Users/zh355245849/Desktop/project7/' + link.contents[0])\n",
    "    openurl = urlopen('http://www.careercup.com' + link.attrs['href'])\n",
    "    bs = BeautifulSoup(openurl, \"lxml\")\n",
    "    b = bs\n",
    "    if bs.find('div', {'class':'pageListSection'}).find('span', {'class':'pageNumberActive'}) is not None:\n",
    "        p1 = urlopen(bs.find('div', {'class':'pageListSection'}).find('span', {'class':'pageNumberActive'}).attrs['href'])\n",
    "        r1 = p1.read()\n",
    "        f1 = open('/Users/zh355245849/Desktop/project7/' + link.contents[0] + \"/\" + bs.find('div', {'class':'pageListSection'}).find('span', {'class':'pageNumberActive'}).contents[0] + '.html', 'wb')\n",
    "        f1.write(r1)\n",
    "        f1.close\n",
    "        #if bs.find('div', {'class':'pageListSection'}).findAll('a', {'class':'pageNumber'}):\n",
    "        for i in bs.find('div', {'class':'pageListSection'}).findAll('a', {'class':'pageNumber'}):\n",
    "            p2 = urlopen(i.attrs['href'])\n",
    "            r2 = p2.read()\n",
    "            f2 = open('/Users/zh355245849/Desktop/project7/' + link.contents[0] + \"/\" + i.contents[0] + '.html', 'wb')\n",
    "            f2.write(r2)\n",
    "            f2.close  \n",
    "        if b.find('div', {'class':'pageListSection'}).find('a', {'class':''}) is not None:\n",
    "            url = urlopen(b.find('div', {'class':'pageListSection'}).find('a', {'class':''}).attrs['href'])\n",
    "            bs1 = BeautifulSoup(url, \"lxml\")\n",
    "            b = bs1\n",
    "            p3 = urlopen(bs1.find('div', {'class':'pageListSection'}).find('span', {'class':'pageNumberActive'}).attrs['href'])\n",
    "            r3 = p3.read()\n",
    "            f3 = open('/Users/zh355245849/Desktop/project7/' + link.contents[0] + \"/\" + bs1.find('div', {'class':'pageListSection'}).find('span', {'class':'pageNumberActive'}).contents[0] + '.html', 'wb')\n",
    "            f3.write(r3)\n",
    "            f3.close\n",
    "            for i in bs1.find('div', {'class':'pageListSection'}).findAll('a', {'class':'pageNumber'}):\n",
    "                p4 = urlopen(i.attrs['href'])\n",
    "                r4 = p4.read()\n",
    "                f4 = open('/Users/zh355245849/Desktop/project7/' + link.contents[0] + \"/\" + i.contents[0] + '.html', 'wb')\n",
    "                f4.write(r4)\n",
    "                f4.close\n",
    "    else:\n",
    "        openurl = urlopen('http://www.careercup.com' + link.attrs['href'])\n",
    "        #can not use beautifulSoap before read page\n",
    "        page = openurl.read()\n",
    "        f = open('/Users/zh355245849/Desktop/project7/' + link.contents[0] + \"/\" + link.contents[0] + '.html', 'wb')\n",
    "        f.write(page)\n",
    "        f.close\n",
    "#     if not os.path.exists('/Users/zh355245849/Desktop/project5/' + link.find('a').contents[0]):\n",
    "#         os.makedirs('/Users/zh355245849/Desktop/project5/' + link.find('a').contents[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
